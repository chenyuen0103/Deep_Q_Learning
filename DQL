import gym
import torch
import torch.nn.functional as F
from torch import nn, optim
from collections import deque


class Memory ():
    def __init__(self , max_size = 1000):
    self.buffer = deque(maxlen=max_size)
    def add(self , experience):
    self.buffer.append(experience)
    def sample(self , batch_size):
    idx = np.random.choice(np.arange(len(self.buffer)), 
                           size=batch_size,
                           replace=False)
    return [self.buffer[ii] for ii in idx]
    
    
env = gym.make('CartPole-v0')
action_size = 2
state_size = 4

memory_size = 10000
pretrain_length = 100
batch_size = 100

# Initialize the simulation
env.reset()
# Take one random step to get the pole and cart moving
state , reward , done , _ = env.step(env.action_space.sample ())

memory = Memory(max_size=memory_size)


# Make a bunch of random actions and store the experiences
for _ in range(pretrain_length):
    # Do a random action
    action = env.action_space.sample ()
    next_state , reward , done , _ = env.step(action)
    if done:
    # Simulation fails , so no next state
    next_state = np.zeros(state.shape)
    # Add experience to memory
    memory.add((state , action , reward , next_state, done))
    # Start new episode
    env.reset()
    # Take one random step to get the pole and cart moving
    state , reward , done , _ = env.step(env.action_space.sample ())
    else:
    # Add experience to memory
    memory.add((state , action , reward , next_state, done))
    state = next_state
    
    
    
    
class nnModel(nn.Module):
    def __init__(self,state_size, action_size):
        super().__init__()
        self.L1 = nn.Linear(state_size,64, bias = True)
        self.L2 = nn.Linear(64,64, bias = True)
        self.L3 = nn.Linear(64,action_size, bias = True)

    def forward(self,s):
    lr = 0.01
    s = torch.from_numpy(s).float()
    s = torch.relu(self.L1(s))
    s = torch.relu(self.L2(s))
    s = self.L3(s)
    return s


def MSE(input,target):
    return torch.mean((torch.tensor(target,requires_grad=True) - torch.tensor(input,requires_grad=True))**2)
    
    
    
def DQL(no_eposide, batch_size, gamma, eps_max = 1, eps_min = 0.005, alpha = 0.025):
    Q = nnModel(state_size, action_size)
    Q = Q.float()
    optimizer = optim.Adam(Q.parameters(), lr=0.01)
    total_reward_list = []
    t = 0

    env.reset()
    for ep in tqdm(range(no_eposide)):
    optimizer.zero_grad()
    s,a,reward,s_new,done = memory.sample(1)[0]
    env.reset()
    r = 0


    while not done and r <= 200:
        eps = eps_min+(eps_max-eps_min)*np.exp(-alpha*t)
        temp = np.random.uniform()
        #a_t = env.action_space.sample()
        if temp < eps:
        a = env.action_space.sample()
        else:
        a = torch.argmax(Q(s)).item()

        t += 1 

        next_state , reward , done , _ = env.step(a)
        r += reward
        memory.add((s , a, reward , next_state, done))
        s = next_state
        if done:
        env.reset()

        batch = memory.sample(batch_size)
        current_lst = []
        target_lst = []

        for sb,ab,rb,s_newb,done in batch:
        current_lst.append(Q(sb)[ab])
        target_lst.append(rb + int(1-done)*(gamma * torch.max(Q(s_newb))))

        optimizer.zero_grad()
        loss = MSE(current_lst, target_lst)
        loss.backward()
        optimizer.step()



    total_reward_list.append((ep, r))


    return Q, total_reward_list
    
    
Q,R = DQL(1000, batch_size, gamma = 0.99)



# Moving average for smoothing plot
def running_mean(x, N):
    cumsum = np.cumsum(np.insert(x, 0, x[0]*np.ones(N)))
    return (cumsum[N:] - cumsum[:-N]) / N

eps, rews = np.array(R).T
smoothed_rews = running_mean(rews, 10)
plt.plot(eps, smoothed_rews,label ='Running avg')
plt.plot(eps, rews, color='grey', alpha=0.3,label = 'Reward')
plt.title('Accumulated Reward per Episode')
plt.xlabel('Episode')
plt.ylabel('Accumulated Reward')
